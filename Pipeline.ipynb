{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt ZTNBD\n",
    "\n",
    "## Lokalna instalacja\n",
    "###### Wymagania\n",
    "- docker\n",
    "- ~4GB wolnego miejsca na dysku\n",
    "- zalecany linux + build-essential\n",
    "\n",
    "###### Linux \n",
    "```bash\n",
    "make notebook\n",
    "```\n",
    "###### Others (nie sprawdzane)\n",
    "```bash\n",
    "# lokalizacja - główny katalogu projektu\n",
    "docker run -it --rm -v $(pwd):/home/jovyan/work -p 8888:8888 jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "Komenda startuje kontener dockerowy z Jupyterem i podmontowuje katalog projektu.\n",
    "Moduły znajdują się w katalogu `modules` i tam też będą lądowały kolejne.\n",
    "Uruchomienie póki co możliwe jest tylko z poziomu notebook'a.\n",
    "\n",
    "[comment - nieaktualny dokument]: <> (+ Dokumentacja google [Google Docs](https://docs.google.com/document/d/1IylTvJbRe8s_j_bZqbM-6nWVa2IQDjQiZx-NyJsGZbg))\n",
    "\n",
    "## Moduły\n",
    "\n",
    "\n",
    "### Posty\n",
    "\n",
    "##### PostTransformer\n",
    "Klasa PostTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta wydobywa z kolumny inputCol, z formatu json, content i umieszcza go w kolumnie outputCol w postaci tekstu.\n",
    "\n",
    "##### TranslateTransformer\n",
    "Klasa TranslateTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta tłumaczy tekst zawarty w kolumnie inputCol z języka polskiego na angielski i umieszcza go w kolumnie outputCol.\n",
    "\n",
    "##### SentenceTransformer\n",
    "Klasa SentenceTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta dzieli tekst zawarty w kolumnie inputCol na zdania i umieszcza go w kolumnie outputCol w postaci tablicy tekstów.\n",
    "\n",
    "##### SpeechPartsTransformer\n",
    "Klasa SpeechPartsTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta z listy tekstów zawartych w kolumnie inputCol, zlicza wystąpienia części mowy i wstawia do outputCol w postaci słownika, którego kluczem jest tag części mowy wg projektu [Penn Treebank](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), a wartością lista ilości wystąpień danej części mowy w danym tekście.\n",
    "\n",
    "##### SentimentTransformer\n",
    "Klasa SentimentTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta z listy tekstów zawartych w kolumnie inputCol, wylicza sentymenty tekstów i wstawia do outputCol w postaci słownika, którego kluczami są polaryzacja oraz subiektywność, a wartościami listy wartości odpowiednich pól sentymentów dla danego tekstu.\n",
    "\n",
    "\n",
    "### Featury\n",
    "\n",
    "##### FeatureTransformer\n",
    "Klasa FeatureTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol,\n",
    "pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe.\n",
    "Metoda ta wydobywa zawartość z kolumny inputCol, z formatu json i umieszcza go w kolumnie outputCol w postaci słownika, którego kluczem jest nazwa featura, a wartością lista wartości danego featura, w kolejnych elementach jsonowych.\n",
    "\n",
    "\n",
    "### Uniwersalne\n",
    "\n",
    "##### MaxTransformer\n",
    "Klasa MaxTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol,\n",
    "pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda\n",
    "ta wylicza maksimum z listy wartości, dla wszystkich kluczy znajdujących się w słowniku zawartym w kolumnie inputCol. Wyliczone wartości wstawia do kolumny outputCol w postaci słownika klucz - maksimum wartości.\n",
    "\n",
    "#####  MeanTransformer\n",
    "Klasa MeanTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol,\n",
    "pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda\n",
    "ta wylicza średnią z listy wartości, dla wszystkich kluczy znajdujących się w słowniku zawartym w kolumnie inputCol. Wyliczone wartości wstawia do kolumny outputCol w postaci słownika klucz - średnia wartości.\n",
    "\n",
    "##### MedianTransformer\n",
    "Klasa MedianTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol,\n",
    "pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda\n",
    "ta wylicza medianę z listy wartości, dla wszystkich kluczy znajdujących się w słowniku zawartym w kolumnie inputCol. Wyliczone wartości wstawia do kolumny outputCol w postaci słownika klucz - wartość środkowa.\n",
    "\n",
    "##### NumberOfOccurrencesTransformer\n",
    "Klasa NumberOfOccurrencesTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol,\n",
    "pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda\n",
    "ta wylicza długość listy wartości, dla wszystkich kluczy znajdujących się w słowniku zawartym w kolumnie inputCol. Wyliczone wartości wstawia do kolumny outputCol w postaci słownika klucz - liczba elementów listy wartości.\n",
    "\n",
    "##### ConvertDictToVectorTransformer\n",
    "Klasa ConvertDictToVectorTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol,    pyspark.ml.param.shared.HasOutputCol. Klasa ta przyjmuje dodatkowe parametry: keys, który zawiera listę nazw kluczy,\n",
    "które mają zostać skonwertowane w podanej kolejności do formatu wektora oraz element_type, który określa typ elementu słownika. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta wydobywa zawartość z kolumny inputCol, w formacie słownika i umieszcza go w kolumnie outputCol w postaci wektora wartości kluczy podanych w parametrze. W przypadku braku istnienia danego klucza w słowniku, zwracana wartość będzie równa None.\n",
    "\n",
    "##### SelectRecordsTransformer\n",
    "Klasa SelectRecordsTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol,\n",
    "pyspark.ml.param.shared.HasOutputCol. Klasa ta przyjmuje dodatkowe parametry: keys, który zawiera listę nazw kluczy,\n",
    "które mają zostać skonwertowane w podanej kolejności do formatu listy oraz element_type, który określa typ elementu słownika. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta wydobywa zawartość z kolumny inputCol, w formacie słownika i umieszcza go w kolumnie outputCol w postaci słownika z przefiltrowanymi kluczami podanymi w parametrze. W przypadku braku istnienia danego klucza w słowniku, zwracana wartość będzie równa None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład użycia\n",
    "\n",
    "### Instalacja Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/conda/lib/python3.6/site-packages (from textblob)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk>=3.1->textblob)\n",
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!python -m textblob.download_corpora lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicjalizacja środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import ArrayType, IntegerType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from modules.posts import (\n",
    "    SentenceTransformer,\n",
    "    PostTransformer,\n",
    "    TranslateTransformer,\n",
    "    SpeechPartsTransformer,\n",
    "    SentimentTransformer\n",
    ")\n",
    "from modules.features_ import (\n",
    "    FeatureTransformer\n",
    ")\n",
    "from modules.universal import (\n",
    "    ConvertDictToVectorTransformer,\n",
    "    SelectRecordsTransformer,\n",
    "    MaxTransformer,\n",
    "    MeanTransformer,\n",
    "    MedianTransformer,\n",
    "    NumberOfOccurrencesTransformer,\n",
    ")\n",
    "sc = SparkContext('local[*]', 'PipelineFlow')\n",
    "sess = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytywanie plików"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(spark_ctx, files):\n",
    "    rdd = spark_ctx.wholeTextFiles(files)\n",
    "    rdd = rdd.map(lambda x: (x[0], x[1]))\n",
    "    df = rdd.toDF(['file', 'content'])\n",
    "    return df\n",
    "\n",
    "def load_posts(spark_ctx, files):\n",
    "    rdd = spark_ctx.wholeTextFiles(files)\n",
    "    rdd = rdd.map(lambda x: (x[0], json.loads(x[1])))\n",
    "    df = rdd.toDF(['file', 'content'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykłady użycia transformerów z pipeline'ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(features_as_df):\n",
    "    \n",
    "    features = [\n",
    "        \"leaf\",\n",
    "        \"has-attribute-class\",\n",
    "    ]\n",
    "    \n",
    "    featurer = FeatureTransformer();\n",
    "    featurer.setInputCol('content').setOutputCol('features')\n",
    "    \n",
    "    feature_selector = SelectRecordsTransformer(keys=features, element_type=ArrayType(DoubleType()))\n",
    "    feature_selector.setInputCol('features').setOutputCol('selected_features')\n",
    "    \n",
    "    max_feature_transformer = MaxTransformer()\n",
    "    max_feature_transformer.setInputCol('selected_features').setOutputCol('max')\n",
    "    \n",
    "    mean_feature_transformer = MeanTransformer()\n",
    "    mean_feature_transformer.setInputCol('selected_features').setOutputCol('mean')\n",
    "    \n",
    "    median_feature_transformer = MedianTransformer()\n",
    "    median_feature_transformer.setInputCol('selected_features').setOutputCol('median')\n",
    "    \n",
    "    number_of_occurences_feature_transformer = NumberOfOccurrencesTransformer()\n",
    "    number_of_occurences_feature_transformer.setInputCol('selected_features').setOutputCol('occurences')\n",
    "    \n",
    "    dict_to_vector_converter = ConvertDictToVectorTransformer(keys=features)\n",
    "    dict_to_vector_converter.setInputCol('occurences').setOutputCol('occurences_vector')\n",
    "    \n",
    "    stages = [\n",
    "        featurer,\n",
    "        feature_selector,\n",
    "        max_feature_transformer,\n",
    "        mean_feature_transformer,\n",
    "        median_feature_transformer,\n",
    "        number_of_occurences_feature_transformer,\n",
    "        dict_to_vector_converter\n",
    "    ]\n",
    "        \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipeline = pipeline.fit(features_as_df)\n",
    "    pipeline = pipeline.transform(features_as_df)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def transform_posts(posts_as_df):\n",
    "    \n",
    "    poster = PostTransformer()\n",
    "    poster.setInputCol('content').setOutputCol('posts')\n",
    "    \n",
    "    translator = TranslateTransformer()\n",
    "    translator.setInputCol('posts').setOutputCol('translated')\n",
    "    \n",
    "    sentencer = SentenceTransformer()\n",
    "    sentencer.setInputCol('translated').setOutputCol('sentences')\n",
    "    \n",
    "    speech_parter = SpeechPartsTransformer()\n",
    "    speech_parter.setInputCol('translated').setOutputCol('speechParts')\n",
    "    \n",
    "    sentimenter = SentimentTransformer()\n",
    "    sentimenter.setInputCol('translated').setOutputCol('sentiments')\n",
    "\n",
    "    tags = [\n",
    "        'NN',\n",
    "        'NNS',\n",
    "        'NNPS'\n",
    "    ]\n",
    "    \n",
    "    speech_parts_selector = SelectRecordsTransformer(keys=tags, element_type=ArrayType(IntegerType()))\n",
    "    speech_parts_selector.setInputCol('speechParts').setOutputCol('nouns')\n",
    "    \n",
    "    max_nouns_transformer = MaxTransformer()\n",
    "    max_nouns_transformer.setInputCol('nouns').setOutputCol('max')\n",
    "    \n",
    "    mean_nouns_transformer = MeanTransformer()\n",
    "    mean_nouns_transformer.setInputCol('nouns').setOutputCol('mean')\n",
    "    \n",
    "    median_nouns_transformer = MedianTransformer()\n",
    "    median_nouns_transformer.setInputCol('nouns').setOutputCol('median')\n",
    "\n",
    "    dict_to_vector_converter = ConvertDictToVectorTransformer(keys=tags)\n",
    "    dict_to_vector_converter.setInputCol('mean').setOutputCol('mean_vector')\n",
    "    \n",
    "    stages = [\n",
    "        poster,\n",
    "        translator, \n",
    "        sentencer, \n",
    "        speech_parter,\n",
    "        sentimenter,\n",
    "        speech_parts_selector,\n",
    "        max_nouns_transformer,\n",
    "        mean_nouns_transformer,\n",
    "        median_nouns_transformer,\n",
    "        dict_to_vector_converter\n",
    "    ]\n",
    "    \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipeline = pipeline.fit(posts_as_df)\n",
    "    pipeline = pipeline.transform(posts_as_df)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(selected_features={'leaf': [1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 2.0], 'has-attribute-class': [1.0, 1.0, 1.0, 1.0]}, max={'leaf': 2.0, 'has-attribute-class': 1.0}, mean={'leaf': 1.1428571428571428, 'has-attribute-class': 1.0}, median={'leaf': 1.0, 'has-attribute-class': 1.0}, occurences={'leaf': 7, 'has-attribute-class': 4}, occurences_vector=DenseVector([7.0, 4.0]))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_file = 'data/featuresample.json'\n",
    "loaded_features = load_features(sc, feature_file)\n",
    "\n",
    "result = transform_features(loaded_features)\n",
    "             \n",
    "result.select('selected_features','max','mean', 'median', 'occurences', 'occurences_vector').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sentences=['Oh, please ... very interesting product.', \"I've never had contact with such paint.\", 'Regards ;)', 'It gives a yellow effect - how will it look like a natural bright bladder?', 'Regards', 'Stupid question but whether the conditioner with bimatoporost applied to the eyebrows penetrate the skin?', 'Can it have any consequences for your eyesight?', 'I do not put on eyelashes.', 'I have seen this product once.', 'Is very interesting.', 'For me, he could only act as he says :-) beautiful hair :-)', 'I did not try cassia, but I used Sense 2 times :) the effects were similar to yours :)', \"It's great that there is a post with you, I have Cassia, but they write about its properties in different ways\", \"Paint is probably too big a word - it's more a conditioner;)\", 'I have no clue - it all depends on what shade you have.', 'You can even include visual darkening (which happened to me with quite natural hair).', 'Probably not, but you can get a bit of \"darkening\" the skin under the eyebrows.', 'Thank you for your answer', 'Nutrition also works great;)', 'I tried to suck myself, but I prefer xD powder', 'I had it in the grinder, just the senna is very simple to grind :)', 'Life issue of missing xD mill', 'What, for example, did you find a strange?', ';)'], speechParts={'NN': [3, 3, 5, 2, 2, 1, 3, 4, 2, 1, 1, 1, 2, 3, 2], 'JJ': [2, 2, 1, 2, 1, 2, 1, 2, 0, 0, 1, 1, 1, 0, 0], 'WRB': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'LS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'PRP': [1, 2, 2, 4, 2, 4, 1, 5, 1, 1, 0, 3, 2, 0, 1], 'DT': [0, 2, 4, 1, 1, 1, 2, 2, 3, 0, 0, 0, 2, 0, 1], 'NNP': [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0], 'FW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'NNS': [1, 1, 3, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'JJS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'JJR': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'UH': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'MD': [0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], 'VBD': [0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1], 'WP': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], 'VBG': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], 'CC': [0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0], 'CD': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'PDT': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'RBS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'RBR': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'VBN': [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'IN': [1, 1, 4, 2, 0, 4, 0, 2, 2, 1, 0, 0, 1, 1, 1], 'VBP': [1, 0, 2, 1, 0, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0], 'SYM': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'WDT': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 'NNPS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'WP$': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'VB': [1, 1, 2, 1, 2, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1], 'VBZ': [0, 1, 0, 2, 0, 2, 2, 1, 0, 0, 1, 0, 1, 0, 0], 'RB': [2, 0, 1, 3, 1, 0, 2, 2, 2, 0, 1, 0, 2, 0, 0], 'EX': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'PRP$': [0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], 'POS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'TO': [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0], 'RP': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, sentiments={'subjectivity': [0.7166666666666667, 0.39999999999999997, 1.0, 0.93, 0.7999999999999999, 0.675, 0.5333333333333333, 0.2, 0.0, 0.0, 0.875, 0.0, 0.7321428571428572, 0.05, 0.575], 'polarity': [0.3, 0.26666666666666666, -0.7999999999999999, 0.5, 0.3333333333333333, 0.4, 0.25, 0.05, 0.0, 0.0, 0.525, 0.0, 0.25, -0.2, 0.1]}, nouns={'NN': [3, 3, 5, 2, 2, 1, 3, 4, 2, 1, 1, 1, 2, 3, 2], 'NNS': [1, 1, 3, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'NNPS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, max={'NN': 5.0, 'NNS': 3.0, 'NNPS': 0.0}, mean={'NN': 2.3333333333333335, 'NNS': 0.6666666666666666, 'NNPS': 0.0}, median={'NN': 2.0, 'NNS': 0.0, 'NNPS': 0.0}, mean_vector=DenseVector([2.2667, 0.6667, 0.0]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_files = 'data/posts/*'\n",
    "loaded_posts = load_posts(sc, post_files)\n",
    "\n",
    "post_pipeline = transform_posts(loaded_posts)\n",
    "post_pipeline.select('sentences','speechParts','sentiments','nouns','max','mean', 'median', 'mean_vector').take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
